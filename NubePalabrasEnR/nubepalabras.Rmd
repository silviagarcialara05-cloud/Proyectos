---
title: "TEXTO. NUBES DE PALABRAS"
author: "Silvia Garcia Lara, Marina Palomino Duque y Sheila San Jose Baraja"
date: "11 de mayo de 2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, echo = TRUE, message=FALSE)
```
```{r, echo=FALSE}
setwd('D:/Users')
```


Una nube de palabras o nube de etiquetas es una representaciÃ³n visual de las palabras que conforman un texto, en donde el tamaÃ±o es mayor para las palabras que aparecen con mÃ¡s frecuencia

#Primera Parte

1.- Instalar los paquetes tm, wordcloud y wordcloud2 en R. Â¿Para que sirven?
  
```{r, eval=FALSE}
#Instalamos los paquetes requeridos mediante la instruccion install.packages() que nos proporciona R 
install.packages("tm")
install.packages("wordcloud")
install.packages("wordcloud2")

#Para saber para que sirve cada uno de los paquetes instalados anteriormente, hacemos uso de internet y de la instruccion help() que nos proporciona R:
help(tm)
help(wordcloud)
help(wordcloud2)
```

Las funciones de cada paquete son:
  
-tm: brinda un marco para realizar aplicaciones de mineria de texto en R

-wordcloud: sirve para realizar nubes de palabras, siendo uno de los metodos mas comunes dentro de la mineria de texto (text mining) para resaltar visualmente las palabras mas frecuentes en un texto. 

Parametros: 

words: vector donde se encuentran las palabras.
freq: vector con la frecuencia de cada una de las palabras.
scale: indica el rango del tamano de las palabras.
min.freq: la frecuencia minima de las palabras que queremos tratar.
max.words: numero maximo de palabras que queremos tratar.
random.order: dibuja las palabras aleatoriamente. Si es falso se dibujan de mayor a menor frecuencia.
random.color: elige los colores al azar. Si es falso se eligen en funcion de la frecuencia.
rot.per: proporcion de palabras con una rotacion de 90 grados.
colors: los colores que van a aparecer en el grafico.
ordered.colors: si es verdadero los colores se asignan a las palabras en orden.

-wordcloud2: al igual que wordcloud, sirve para realizar nubes de palabras, anadiendo algunas funciones a mayores. 

Parametros:

data: data frame incluyendo las palabras y la frecuencia de cada una en cada columna. 
size: tamano de la fuente. Por defecto vale 1. 
fontFamily: fuente de las palabras.
fontWeight: peso de la fuente de las palabras.
color: sistema de color de fuente. Puede elegir "random-dark" o "random-light".
backgroundColor: Color del fondo. 
minRotation y maxRotation: el valor minimo y maximo del angulo de rotacion del texto.
rotateRatio: relacion de rotacion de fuente, si se establece en 1, todas las palabras se 
rotaran.
shape: la forma de la nube de palabras. Las opciones disponibles son: "circulo" , "estrella" , "cardioide", "diamante" , "triangulo- hacia adelante "," triangulo ","pentagono ",...
figPath: figura usada para la nube de palabras.


2.- Describe con tus palabras para que sirven las ordenes del codigo siguiente.

```{r, eval=FALSE}
plain.text<-readLines("Documento.txt", warn=F) #Leemos el texto Documento.txt linea por linea.
library(tm) #Cargamos la libreria "tm".       
myCorpus = VCorpus(VectorSource(plain.text)) #Creacion del objeto myCorpus de tipo VCorpus previamente habiendo aplicado VectorSource al texto leido.
myCorpus = tm_map(myCorpus, tolower) #Todas las palabras se escriben en minusculas.
myCorpus = tm_map(myCorpus, removePunctuation) #Eliminacion de los signos de puntuacion.
myCorpus = tm_map(myCorpus, removeNumbers) #Eliminacion de los numeros.
myCorpus = tm_map(myCorpus, removeWords, stopwords("spanish")) #Eliminar palabras que usamos con mucha frecuencia pero que no tienen un significado concreto en espanol.
myCorpus <- tm_map(myCorpus, PlainTextDocument) #Elimina el formato. 
myDTM = TermDocumentMatrix(myCorpus, control = list(minWordLength = 1)) #Convierte el corpus en una tabla de frecuencia considerando palabras con una longitud minima de 1 caracter
m = as.matrix(myDTM) #Transformamos myDTM en una matriz.
v = sort(rowSums(m), decreasing = TRUE) #Se ordena de manera decreciente el sumatorio de las filas de m.
library(wordcloud) #Cargamos la libreria wordcloud.
wordcloud(names(v),v, min.freq = 2, colors=brewer.pal(6,"Dark2"),  random.order=FALSE) #Creacion de una nube de palabras seleccionando como palabras los nombres y las frecuencias que contiene v , eligiendo aquellos que tengan una frecuencia igual o mayor que 2 y usando  como colores de las palabras la paleta brewer(6, Dark2). Por ultimo se ordenan las palabras dependiendo de la frecuencia de cada una.

```

3.- Considerar el texto que se encuentra en el archivo Texto.txt que corresponde al articulo publicado en el periodico eldiario.es titulado El gigante estadounidense del carbon financiaba a decenas de grupos que niegan el cambio climatico.

  + A partir de Texto.txt mostrar un objeto que contenga las palabras que aparezcan 10 o mas veces en el texto (que no sean conectores ni articulos, etc propios de la gramatica) junto con la frecuencia de cada una de ellas. 
Realizar un grafico de barras con estas palabras. Realizar una nube de palabras.

```{r}

#Cargamos las librerias tm  , wordcloud, wordcloud2, "NLP" y "RColorBrewer"
library(NLP)
library(tm)
library(RColorBrewer)
library(wordcloud)
library(wordcloud2)

#Leemos el texto linea por linea:
texto<-readLines("Texto.txt", warn=F)

#Acondicionamos el texto poniendo las palabras en minuscula, eliminando los signos de puntuacion, los numeros y las palabras que se usan con bastante frecuencia en el espanol pero que no tienen un significado relevante
Corpus = VCorpus(VectorSource(texto))
Corpus = tm_map(Corpus, tolower)
Corpus = tm_map(Corpus, removePunctuation)
Corpus = tm_map(Corpus, removeNumbers)
Corpus = tm_map(Corpus, removeWords, stopwords("spanish"))
Corpus <- tm_map(Corpus, PlainTextDocument)
DTM = TermDocumentMatrix(Corpus, control = list(minWordLength = 1))

#Convertimos el texto resultante a matriz y ordenamos las palabras de mayor a menor frecuencia
m = as.matrix(DTM)
v = sort(rowSums(m), decreasing = TRUE)

#Creamos un data frame con las palabras del documento y la frecuencia con la que aparecen en el 
mywords <- data.frame(words = names(v), freq = v) 
rownames(mywords)<-NULL
#Creamos otro data frame con aquellas palabras cuya frecuencia es superior a 10
mywords2 <- mywords[mywords$freq > 10,]
```

Realizamos un grafico de barras con estas palabras
```{r}
barplot(height = mywords2$freq, names = mywords2$words, col=c('dodgerblue4','dodgerblue3','dodgerblue2','dodgerblue1','dodgerblue'))

```

Realizamos la nube de palabras para el conjunto de datos
```{r}
wordcloud(mywords2$words,mywords2$freq,min.freq=1,random.order=F,colors =brewer.pal(8, "Accent"))

```

  + Realizar varias nubes de palabras que consideres de interes. En todos los casos muestra la tabla de frecuencias correspondiente.

```{r}
#Palabras que tienen una frecuencia mayor que 3 usando unicamente como colores el rojo y el negro y haciendo que la nube de palabras creada tenga forma de nube.
mayor3<-mywords[mywords$freq>3,]
mayor3
wordcloud(mayor3$words, mayor3$freq, min.freq=1, random.order=F, color = rep_len(c('Black','Red'), nrow(mayor3)))

#Palabras que tienen una frecuencia entre 3 y 10, en forma de nube
entre3y10<-mywords[mywords$freq>3 & mywords$freq<10,]
entre3y10
wordcloud(entre3y10$words, entre3y10$freq, random.order=F, scale=c(2,0.05),colors=brewer.pal(8, "Set1"))

#Las 15 palabras mas frecuentes en forma de nube
frecuentes<-head(mywords,15)
frecuentes
wordcloud(frecuentes$words, frecuentes$freq, min.freq=1,scale=c(3, 0.25), colors=brewer.pal(8, "Set1"))

#Las 10 palabras menos frecuentes en forma de nube
menosfreq<-tail(mywords,10)
menosfreq
wordcloud(menosfreq$words,menosfreq$freq,min.freq=1,random.order=T,scale=c(3,0.25), colors=brewer.pal(8, "Set1"))

#Palabras que tienen una  frecuencia igual a 2 , creando la nube con  forma de estrella
freq2<-mywords[mywords$freq==2,]
freq2
wordcloud2(freq2,backgroundColor = "white",shape='star',size=0.1)
```


4.- Realizar una nube de palabras para mostrar las 10 palabras mas frecuentes en cada una de las hojas de ejercicios sobre R de este curso. Luego hacer una unica nube de palabras con las 10 palabras mas frecuentes en todas esas hojas de ejercicios.

```{r, eval=FALSE}
#instalamos el paquete pdftools para poder leer los pdf
install.packages('pdftools')
````

Realizamos la nube de palabras mostrando las 10 palabras mas frecuentes en cada una de las hojas de ejercicios sobre R de este curso:

```{r}

#Cargamos las librerias 'tm', 'wordcloud', 'NLP', y 'wordcloud2' necesarias para poder realizar nubes de palabras

library(tm)
library(NLP)
library(RColorBrewer)
library(wordcloud)
library(wordcloud2)
library(pdftools)
files <- list.files(pattern = "pdf$")
corp <- Corpus(URISource(files),
               readerControl = list(reader = readPDF))
```

Seleccionamos cada una de las hojas y realizamos su nube de palabras correspondiente

Hoja 1:
```{r}
corp_hoja1<-corp[1]
#Acondicionamos el texto de la hoja 1 poniendo las palabras en minuscula, eliminando los signos de puntuacion, los numeros y las palabras que se usan con bastante frecuencia en el espanol pero que no tienen un significado relevante
corp_hoja1 = tm_map(corp_hoja1, tolower)
corp_hoja1 = tm_map(corp_hoja1, removePunctuation)
corp_hoja1 = tm_map(corp_hoja1, removeNumbers)
corp_hoja1 = tm_map(corp_hoja1, removeWords, stopwords("spanish"))
corp_hoja1 <- tm_map(corp_hoja1, PlainTextDocument)
DTM1 = TermDocumentMatrix(corp_hoja1, control = list(minWordLength = 1))
#Convertimos el texto resultante a matriz y ordenamos las palabras de mayor a menor frecuencia
m1 = as.matrix(DTM1)
v1= sort(rowSums(m1), decreasing = TRUE)

#Creamos un data frame con las palabras del documento y la frecuencia con la que aparecen en el 
hoja1 <- data.frame(words = names(v1), freq = v1)
rownames(hoja1)<-NULL
nube1<-head(hoja1,10)

#Realizamos una nube de palabras con las palabras anteriores
wordcloud(nube1$words, nube1$freq, min.freq=1, random.order=F,colors=brewer.pal(8, "Dark2"))
```

Hoja 2:
```{r}
corp_hoja2<-corp[2]
#Acondicionamos el texto de la hoja 2 poniendo las palabras en minuscula, eliminando los signos de puntuacion, los numeros y las palabras que se usan con bastante frecuencia en el espanol pero que no tienen un significado relevante
corp_hoja2 = tm_map(corp_hoja2, tolower)
corp_hoja2 = tm_map(corp_hoja2, removePunctuation)
corp_hoja2 = tm_map(corp_hoja2, removeNumbers)
corp_hoja2 = tm_map(corp_hoja2, removeWords, stopwords("spanish"))
corp_hoja2 <- tm_map(corp_hoja2, PlainTextDocument)
DTM2 = TermDocumentMatrix(corp_hoja2, control = list(minWordLength = 1))
#Convertimos el texto resultante a matriz y ordenamos las palabras de mayor a menor frecuencia
m2 = as.matrix(DTM2)
v2 = sort(rowSums(m2), decreasing = TRUE)

#Creamos un data frame con las palabras del documento y la frecuencia con la que aparecen en el 
hoja2 <- data.frame(words = names(v2), freq = v2)
rownames(hoja2)<-NULL
nube2<-head(hoja2,10)

#Realizamos una nube de palabras con las palabras anteriores
wordcloud(nube2$words, nube2$freq, min.freq=1, random.order=F,colors=brewer.pal(8, "Dark2"))
```

Hoja 3:
```{r}
corp_hoja3<-corp[3]
#Acondicionamos el texto de la hoja 3 poniendo las palabras en minuscula, eliminando los signos de puntuacion, los numeros y las palabras que se usan con bastante frecuencia en el espanol pero que no tienen un significado relevante
corp_hoja3 = tm_map(corp_hoja3, tolower)
corp_hoja3 = tm_map(corp_hoja3, removePunctuation)
corp_hoja3 = tm_map(corp_hoja3, removeNumbers)
corp_hoja3 = tm_map(corp_hoja3, removeWords, stopwords("spanish"))
corp_hoja3 <- tm_map(corp_hoja3, PlainTextDocument)
DTM3 = TermDocumentMatrix(corp_hoja3, control = list(minWordLength = 1))
#Convertimos el texto resultante a matriz y ordenamos las palabras de mayor a menor frecuencia
m3 = as.matrix(DTM3)
v3 = sort(rowSums(m3), decreasing = TRUE)

#Creamos un data frame con las palabras del documento y la frecuencia con la que aparecen en el 
hoja3 <- data.frame(words = names(v3), freq = v3)
rownames(hoja3)<-NULL
nube3<-head(hoja3,10)

#Realizamos una nube de palabras con las palabras anteriores
wordcloud(nube3$words, nube3$freq, min.freq=1, random.order=F,colors=brewer.pal(8, "Dark2"))
```

Hoja 4:
```{r}
corp_hoja4<-corp[4]
#Acondicionamos el texto de la hoja 4 poniendo las palabras en minuscula, eliminando los signos de puntuacion, los numeros y las palabras que se usan con bastante frecuencia en el espanol pero que no tienen un significado relevante
corp_hoja4 = tm_map(corp_hoja4, tolower)
corp_hoja4 = tm_map(corp_hoja4, removePunctuation)
corp_hoja4 = tm_map(corp_hoja4, removeNumbers)
corp_hoja4 = tm_map(corp_hoja4, removeWords, stopwords("spanish"))
corp_hoja4 <- tm_map(corp_hoja4, PlainTextDocument)
DTM4 = TermDocumentMatrix(corp_hoja4, control = list(minWordLength = 1))
#Convertimos el texto resultante a matriz y ordenamos las palabras de mayor a menor frecuencia
m4 = as.matrix(DTM4)
v4 = sort(rowSums(m4), decreasing = TRUE)

#Creamos un data frame con las palabras del documento y la frecuencia con la que aparecen en el 
hoja4<- data.frame(words = names(v4), freq = v4)
rownames(hoja4)<-NULL
nube4<-head(hoja4,10)

#Realizamos una nube de palabras con las palabras anteriores
wordcloud(nube4$words, nube4$freq, min.freq=1, random.order=F,colors =  brewer.pal(8, "Dark2"))
```

Hoja 5:
```{r}
corp_hoja5<-corp[5]
#Acondicionamos el texto de la hoja 5 poniendo las palabras en minuscula, eliminando los signos de puntuacion, los numeros y las palabras que se usan con bastante frecuencia en el espanol pero que no tienen un significado relevante
corp_hoja5 = tm_map(corp_hoja5, tolower)
corp_hoja5 = tm_map(corp_hoja5, removePunctuation)
corp_hoja5 = tm_map(corp_hoja5, removeNumbers)
corp_hoja5 = tm_map(corp_hoja5, removeWords, stopwords("spanish"))
corp_hoja5 <- tm_map(corp_hoja5, PlainTextDocument)
DTM5 = TermDocumentMatrix(corp_hoja5, control = list(minWordLength = 1))
#Convertimos el texto resultante a matriz y ordenamos las palabras de mayor a menor frecuencia
m5 = as.matrix(DTM5)
v5 = sort(rowSums(m5), decreasing = TRUE)

#Creamos un data frame con las palabras del documento y la frecuencia con la que aparecen en el 
hoja5<- data.frame(words = names(v5), freq = v5)
rownames(hoja5)<-NULL
nube5<-head(hoja5,10)

#Realizamos una nube de palabras con las palabras anteriores
wordcloud(nube5$words, nube5$freq, min.freq=1, random.order=F, colors= brewer.pal(8, "Dark2"))
```

Hoja 6:
```{r}
corp_hoja6<-corp[6]
#Acondicionamos el texto de la hoja 6 poniendo las palabras en minuscula, eliminando los signos de puntuacion, los numeros y las palabras que se usan con bastante frecuencia en el espanol pero que no tienen un significado relevante
corp_hoja6 = tm_map(corp_hoja6, tolower)
corp_hoja6 = tm_map(corp_hoja6, removePunctuation)
corp_hoja6 = tm_map(corp_hoja6, removeNumbers)
corp_hoja6 = tm_map(corp_hoja6, removeWords, stopwords("spanish"))
corp_hoja6 <- tm_map(corp_hoja6, PlainTextDocument)
DTM6 = TermDocumentMatrix(corp_hoja6, control = list(minWordLength = 1))
#Convertimos el texto resultante a matriz y ordenamos las palabras de mayor a menor frecuencia
m6 = as.matrix(DTM6)
v6 = sort(rowSums(m6), decreasing = TRUE)

#Creamos un data frame con las palabras del documento y la frecuencia con la que aparecen en el 
hoja6<- data.frame(words = names(v6), freq = v6)
rownames(hoja6)<-NULL
nube6<-head(hoja6,10)

#Realizamos una nube de palabras con las palabras anteriores
wordcloud(nube6$words, nube6$freq, min.freq=1, random.order=F, colors= brewer.pal(8, "Dark2"))
```

Hoja 7:
```{r}
corp_hoja7<-corp[7]
#Acondicionamos el texto de la hoja 7 poniendo las palabras en minuscula, eliminando los signos de puntuacion, los numeros y las palabras que se usan con bastante frecuencia en el espanol pero que no tienen un significado relevante
corp_hoja7 = tm_map(corp_hoja7, tolower)
corp_hoja7 = tm_map(corp_hoja7, removePunctuation)
corp_hoja7 = tm_map(corp_hoja7, removeNumbers)
corp_hoja7 = tm_map(corp_hoja7, removeWords, stopwords("spanish"))
corp_hoja7 <- tm_map(corp_hoja7, PlainTextDocument)
DTM7 = TermDocumentMatrix(corp_hoja7, control = list(minWordLength = 1))
#Convertimos el texto resultante a matriz y ordenamos las palabras de mayor a menor frecuencia
m7 = as.matrix(DTM7)
v7 = sort(rowSums(m7), decreasing = TRUE)

#Creamos un data frame con las palabras del documento y la frecuencia con la que aparecen en el 
hoja7<- data.frame(words = names(v7), freq = v7)
rownames(hoja7)<-NULL
nube7<-head(hoja7,10)

#Realizamos una nube de palabras con las palabras anteriores
wordcloud(nube7$words, nube7$freq, min.freq=1, random.order=F, colors=brewer.pal(8, "Dark2"))
```

Hoja 8:
```{r}
corp_hoja8<-corp[8]
#Acondicionamos el texto de la hoja 8 poniendo las palabras en minuscula, eliminando los signos de puntuacion, los numeros y las palabras que se usan con bastante frecuencia en el espanol pero que no tienen un significado relevante
corp_hoja8 = tm_map(corp_hoja8, tolower)
corp_hoja8 = tm_map(corp_hoja8, removePunctuation)
corp_hoja8 = tm_map(corp_hoja8, removeNumbers)
corp_hoja8 = tm_map(corp_hoja8, removeWords, stopwords("spanish"))
corp_hoja8 <- tm_map(corp_hoja8, PlainTextDocument)
DTM8 = TermDocumentMatrix(corp_hoja8, control = list(minWordLength = 1))
#Convertimos el texto resultante a matriz y ordenamos las palabras de mayor a menor frecuencia
m8 = as.matrix(DTM8)
v8 = sort(rowSums(m8), decreasing = TRUE)

#Creamos un data frame con las palabras del documento y la frecuencia con la que aparecen en el 
hoja8<- data.frame(words = names(v8), freq = v8)
rownames(hoja8)<-NULL
nube8<-head(hoja8,10)

#Realizamos una nube de palabras con las palabras anteriores
wordcloud(nube8$words, nube8$freq, min.freq=1, random.order=F, colors= brewer.pal(8, "Dark2"))
```

Realizamos la nube de palabras con las 10 palabras mas frecuentes de todas las hojas de ejercicios del curso
```{r}
#Acondicionamos el texto poniendo las palabras en minuscula, eliminando los signos de puntuacion, los numeros y las palabras que se usan con bastante frecuencia en el espanol pero que no tienen un significado relevante
corp = tm_map(corp, tolower)
corp = tm_map(corp, removePunctuation)
corp = tm_map(corp, removeNumbers)
corp = tm_map(corp, removeWords, stopwords("spanish"))
corp <- tm_map(corp, PlainTextDocument)
DTM = TermDocumentMatrix(corp, control = list(minWordLength = 1))
#Convertimos el texto resultante a matriz y ordenamos las palabras de mayor a menor frecuencia
m = as.matrix(DTM)
v = sort(rowSums(m), decreasing = TRUE)

#Creamos un data frame con las palabras del documento y la frecuencia con la que aparecen en el 
hojas <- data.frame(words = names(v), freq = v)
rownames(hojas)<-NULL
nube<-head(hojas,10)

#Realizamos una nube de palabras con las palabras anteriores
wordcloud(nube$words, nube$freq, min.freq=1, random.order=F, colors=brewer.pal(8, "Dark2"))
```

#Segunda Parte

5.- Algunas ordenes utiles para trabajar con caracteres en R son: nchar , sub, substr, strsplit, regexpr, gregexpr, grep, sub, gsub.

Explicar para que se utilizan y poner un ejemplo cada una de ellas.

```{r, eval = FALSE}
#Para saber para que se utilizan las ordenes del enunciado, hacemos uso de internet y de la instruccion help() que nos proporciona R:
help("nchar")
help("sub")
help("substr")
help("strsplit")
help("regexpr")
help("gregexpr")
help("grep")
help("gsub")
```

-nchar: se le introduce como argumento un vector de tipo caracter y devuelve un vector cuyos elementos contienen el tamano de los correspondientes elementos introducidos previamente como argumentos.

Esta orden puede tener 4 argumentos: 
x = Un vector de caracteres
type = indica como se mide la longitud  del string 
allowNA = (logico) indica si se devuelve un error cuando hay un NA o no
keepNA = (logico) indica si cuando es NA se debe devolver NA o el tamano correspondiente 

Ejemplo: 
```{r}
x <- c("hola", "me", "llamo", NA )
nchar(x,  keepNA= TRUE)
```

-sub: La funcion sub se usa muy a menudo para limpieza de datos. Una llamada a sub tiene la forma sub(x, y, <vector de caracteres>) donde el primer argumento, x, es una expresion regular; la funcion gsub modifica las ocurrencias de esta expresion regular por el segundo argumento, y, en este caso. El tercer argumento es un vector que contiene cadenas de texto en las que se realiza la sustitucion. En caso de que algun elemento del vector de caracteres contenga mas de una ocurrencia del argumento x, solo se modifica la primera vez que aparece, dejando el resto igual.

Ejemplo:
```{r}
df<-c("R is a collaborative project with many contributors")
#reemplaza R por R language
sub('R','R language',df) 
```

-substr: extrae o reemplaza una subcadena en un vector de tipo caracter

Argumentos:
x = un vector de caracteres.
start = el primer elemento a reemplazar.
stop = el ultimo elemento a reemplazar.
value = vector de caracteres que se introduce en la cadena 

Ejemplo: 
```{r}
substr("abcdfg", 3, 6) # extrae la subcadena desde  la posicion 3 a la posicion 6
```

-strsplit: separa los elementos de un vector de tipo caracter en subcadenas de acuerdo con el delimitador proporcionado.

Argumentos:
x = vector de caracteres con los elementos a dividir.
split = vector de caracteres que contiene expresiones para dividir la cadena. Si este tiene longitud 0 el vector x se divide en caracteres.

Ejemplo: 
```{r}
x <- c(as = "asfef", qu = "qwerty", "yuiop[", "b", "stuff.blah.yech")
strsplit(x, "e")
#lo divide por la posicion en la que se encuentran la letra e
```

-regexpr: se usa para identificar donde se encuentra un patron dentro de un vector de caracteres en el cual, cada elemento se busca por separado. Devuelve la posicion de la primera coincidencia con el atributo match.length. Si no encuentra coincidencias devuelve -1. Lo devuelve en forma de vector.

Ejemplo:
```{r}
regexpr("t", "orquesta")
```

-gregexpr: Similar a la funcion regexpr Se usa para identificar donde se encuentra un patron dentro de un vector de caracteres donde cada elemento se busca por separado. Devuelve la informacion en forma de lista. Si no encuentra el patron, devuelve un -1.

Ejemplo:
```{r}
gregexpr("o", "radiologia")
```

-grep: Devuelve que elementos de un vector contienen un patron dado (cadena de caracteres)

Ejemplo:
```{r}
grep("a", c("hola", "arbol","pelo","pelota"))
```

-gsub: La funcion gsub se usa muy a menudo para limpieza de datos. Una llamada a gsub tiene la forma:
gsub(x, y, <vector de caracteres>)
donde el primer argumento, x, es una expresion regular; la funcion gsub modifica las ocurrencias de esta expresion regular por el segundo argumento, y, en este caso. El tercer argumento es un vector que contiene cadenas de texto en las que se realiza la sustitucion.

Ejemplo:
```{r}
gsub("h", "H", c("hola", "buho"))
```

6.- Cada linea del fichero SMSSpam.txt se refiere a un mensaje de texto SMS. Cada linea de dicho fichero esta compuesta de dos columnas:

  + En la primera columna hay una etiqueta spam o ham para indicar si el mensaje es spam o no, respectivamente.
    
  + En la segunda columna se encuentra el contenido del SMS.

Crear un data frame llamado mensajes con dos columnas a partir de este fichero.
 
```{r, eval=FALSE}
#Instalamos y cargamos el paquete "tidyverse"
install.packages("tidyverse")
```

```{r}
library(tidyverse)

#Leemos el archivo de datos de forma que haya dos columnas con las que vamos a trabajar con la siguiente funcion
SMSSpam<- read.fwf("SMSSpam.txt",
                   widths = c(4, 52596)) #Debemos indicar la anchura de cada columna 

#Creacion del data frame mensajes
mensajes<-data.frame(V1 = SMSSpam$V1, V2 = SMSSpam$V3)
```



7.- Anadir una columna mas al data frame mensajes que contenga para cada uno su longitud. ¿Son mas cortos los mensajes spam o los legitimos?

```{r}
#Hallamos la longitud de cada mensaje:
Long<-str_length(mensajes$V2)

#Convertimos V1 en un factor con dos niveles (spam y ham) para trabajar mejor con los datos
Ty<- as.factor(mensajes$V1)

#Anadimos una columna mas con las longitudes y el nuevo factor
mensajes<-data.frame(Tipo = Ty, Contenido = mensajes$V2, Long)

#Calculamos las medias de ambos tipos de mensajes(spam o ham), para comprobar cual de ellos son mas cortos.
x<-mean(mensajes$Long[mensajes$Tipo=="spam"])
x

y<-mean(mensajes$Long[mensajes$Tipo=="ham"])
y
```

Se llega a la conclusion de que son mas cortos los mensajes legitimos que los mensajes spam


8.- Para intentar discernir entre los SMS que son spam y los legitimos, realizar nubes de palabras para los mensajes spam y para los legitimos. ¿Hay grandes diferencias?.

```{r}
#Cargamos los paquetes "tm", "wordcloud" y "wordcloud2" e instalamos los paquetes "tidyverse" y "tidytext" para poder trabajar con ellos.
library(NLP)
library(RColorBrewer)
library(tm)
library(wordcloud)
library(wordcloud2)
#install.packages("tidyverse")
library(tidyverse)
#install.packages("tidytext")
library(tidytext)
```

```{r}
#Guardamos el contenido de los mensajes "Ham" y "Spam" por separado en dos tibbles con dos columnas, una enumerando los mensajes y otra con el contenido de los mensajes.
mensHam <- tibble(parrafo = seq_along(mensajes$Contenido[mensajes$Tipo=="ham"]),
               texto = mensajes$Contenido[mensajes$Tipo=="ham"])

mensSpam <- tibble(parrafo = seq_along(mensajes$Contenido[mensajes$Tipo=="spam"]),
                   texto = mensajes$Contenido[mensajes$Tipo=="spam"])

#Dividimos los mensajes palabra por palabra, cada una asociada al numero de mensaje correspondiente
mens_palSpam <- mensSpam %>%
          unnest_tokens(palabra, texto)

mens_palHam <- mensHam %>%
  unnest_tokens(palabra, texto)

#Contamos el numero de palabras repetidas de cada tipo de mensaje (sort=T para ordenarlas de myor a menor)
frecuenciasS<- mens_palSpam%>% count(palabra, sort =T)

frecuenciasH<- mens_palHam%>% count(palabra, sort =T)
```

```{r}
#Al conocer las frecuencias con las que se repiten las palabras, podemos realizar dos nubes de palabras, para comparar los mensajes, en este caso, escogeremos las 50 palabras mas repetidas de cada tipo de mensaje:
wordcloud(words = frecuenciasS$palabra, 
          freq = frecuenciasS$n, 
          max.words = 50, 
          random.order = F,
          colors=brewer.pal(8, "Dark2"))
```
```{r}
wordcloud(words = frecuenciasH$palabra, 
          freq = frecuenciasH$n, 
          max.words = 50, 
          random.order = F,
          colors=brewer.pal(8, "Dark2"))
```

A simple vista podemos ver que en los mensajes Spam, a diferencia de  las palabras en los mensajes legitimos, las  mas repetidas son algunas como "call" , "free" o "service", que nos hacen ver que son mensajes con un alto contenido de publicidad.

9.- Realiza los resumenes y graficos que consideres interesantes para poder responder a la pregunta ¿hay diferencia entre los mensajes spam o no?

Para poder comparar los dos conjuntos de datos vamos a realizar diferentes graficas apropiadas para ello.

Con respecto al numero de palabras que se repiten podemos ver lo siguiente:

Al  tener las frecuencias con las que se repiten las palabras en ambos tipos de mensaje, la mejor manera de compararlos es observar las palabras mas repetidas de ambos conjuntos de datos para ello creamos data frames con las 6 palabras mas repetidas de cada uno
```{r}
#Creamos los data frames
datosS<- data.frame(count = c(1:6),
                    frec = head(frecuenciasS$n))

datosH<- data.frame(count = c(1:6),
                    frec = head(frecuenciasH$n))
```

Realizamos dos graficos de barras con estos datos 
```{r}
#Grafico 1 para el conjunto de datos "datosS"
ggplot(data = datosS, aes(x=count,y=frec, fill= count)) + 
  geom_col()+
  labs(title ="Palabras mas repetidas en los mensajes Spam")
```

Se observa que la palabra mas repetida en los mensajes Spam tiene una frecuencia muy cercana a 500, mientras que la segunda tiene una frecuencia alrededor de 300. La frecuencia del resto de palabras sigue siendo menor que estas dos, pero ya no se observa una diferencia tan grande en la frecuencia como con estas dos primeras palabras.

```{r}
#Grafico 2
ggplot(data = datosH, aes(x=count,y=frec, fill= count)) + 
  geom_col()+
 labs(title ="Palabras mas repetidas en los mensajes Ham")
```

Al comparar ambos graficos de barras vemos que los mensajes Spam tienen un numero considerablemente mas pequeno de palabras #repetidas que los mensajes Ham.

```{r}
#con un grafico de sectores podemos ver esta diferencia mas claramente respecto a la primera palabra mas repetida de ambos
mayores<- data.frame(mensajes = c("Spam", "Ham"),
                     maximo = c(max(frecuenciasS$n),max(frecuenciasH$n)))

ggplot(mayores, aes(x = "", y = maximo, fill = mensajes)) +
  geom_col() +
  coord_polar(theta = "y")
```

#Tercera Parte

10.- Elige un texto largo de tu interes y realizar nubes de palabras.

Para este apartado, hemos buscado un texto que nos ha parecido bastante interesante ya que trataba sobre un tema de 
actualidad, como es el Covid-19, y, de como este ha repercutido en la salud mental de las personas.

```{r}
#Cargamos las librerias tm, NLP, wordcloud y wordcloud2
library(tm)
library(NLP)
library(wordcloud)
library(wordcloud2)
#Leemos el texto linea por linea
texto<-readLines("Ejercicio_10b.txt", warn=F)

#Acondicionamos el texto poniendo las palabras en minuscula, eliminando los signos de puntuacion, los numeros y las palabras que se usan con bastante frecuencia en el espanol pero que no tienen un significado relevante
Corpus = VCorpus(VectorSource(texto))
Corpus = tm_map(Corpus, tolower)
Corpus = tm_map(Corpus, removePunctuation)
Corpus = tm_map(Corpus, removeNumbers)
Corpus = tm_map(Corpus, removeWords, stopwords("spanish"))
Corpus <- tm_map(Corpus, PlainTextDocument)
DTM = TermDocumentMatrix(Corpus, control = list(minWordLength = 1))
#Convertimos el texto resultante a matriz y ordenamos las palabras de mayor a menor frecuencia
m = as.matrix(DTM)
v = sort(rowSums(m), decreasing = TRUE)

#Creamos un data frame con las palabras del documento y la frecuencia con la que aparecen en el 
mywords <- data.frame(words = names(rowSums(m)), freq = as.numeric(rowSums(m)))

#Nube de palabras con frecuencia mayor que 10
mywords2 <- mywords[mywords$freq > 10,]

#Realizamos una nube de palabras con las palabras anteriores
wordcloud(mywords2$words, mywords2$freq, min.freq=1, random.order=F, colors=brewer.pal(8, "Set1"))


#Nube de palabras con frecuencia mayor de 20
mywords2 <- mywords[mywords$freq > 20,]

#Realizamos la nube de palabras con las palabras anteriores
wordcloud(mywords2$words, mywords2$freq, min.freq=1, random.order=F,colors=brewer.pal(8, "Set1"))


#Nubes de palabras con frecuencia mayor que 30
mywords2 <- mywords[mywords$freq > 30,]

#Realizamos la nube de palabras con las palabras anteriores
wordcloud(mywords2$words, mywords2$freq, min.freq=1, random.order=F,colors=brewer.pal(8, "Set1"))


#Nubes de palabras con frcuencias mayores que 5
mywords2 <- mywords[mywords$freq > 5,]

#Realizamos la nube de palabras con las palabras anteriores
wordcloud(mywords2$words, mywords2$freq, min.freq=1, random.order=F,scale=c(2,0.25),colors=brewer.pal(8, "Set1"))

```

